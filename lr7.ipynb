{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f39e58fa",
   "metadata": {},
   "source": [
    "# Лабораторная работа №7 (Проведение исследований моделями семантической сегментации)\n",
    "## Выполнил студент группы М8О-406Б-21, Орусский В.Р."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f47fcb3",
   "metadata": {},
   "source": [
    "### Выбор исходных данных и обоснование\n",
    "\n",
    "При выполнении данной ЛР использовался датасет [BDD100K](https://www.kaggle.com/datasets/solesensei/solesensei_bdd100k/), аннотирующий изображения для автомобилей с автопилотом. \n",
    "\n",
    "Предназначен для различия объектов на дороге, среди которых выделение тротуаров, дорожных знаков, автомобилей и других различных объектов на дорогах общего пользования. В данном датасете собраны видео с дорожной обстановкой, каждое в среднем по 40 секунд с частотой кадров 30fps (то есть 1200 кадров на видео), снято в разрешении 720p (1280x720px). Помимо этого, в датасет включены данные с GPS, чтобы показать примерную траекторию движения. Датасет охватывает различные погодные условия и времена суток (день / ночь).\n",
    "\n",
    "Датасет выбран для задачи определения объектов в дорожной обстановке, нужно для создания автопилота (продвинутого круиз-контроля)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b624bc7",
   "metadata": {},
   "source": [
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ca6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Data process\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# AI\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Computer Vision\n",
    "from PIL import Image\n",
    "import cv2\n",
    "#import albumentations as A\n",
    "\n",
    "\n",
    "# utils \n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde83764",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q segmentation-models-pytorch\n",
    "%pip install -q torchsummary\n",
    "\n",
    "from torchsummary import summary\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6640752b",
   "metadata": {},
   "source": [
    "Загружаем открытый датасет с помощью `kagglehub`. Сохраняем путь сохранения изображений для дальнейшей работы с выборками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70b0ca78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to auto dataset: C:\\Users\\slava\\.cache\\kagglehub\\datasets\\solesensei\\solesensei_bdd100k\\versions\\2\\bdd100k_seg/bdd100k/seg\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"solesensei/solesensei_bdd100k\")\n",
    "path = os.path.join(path, 'bdd100k_seg/bdd100k/seg')\n",
    "\n",
    "print(\"Path to auto dataset:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f515a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGES = os.path.join(path, 'images/train')\n",
    "TRAIN_MASKS = os.path.join(path, 'labels/train')\n",
    "\n",
    "VAL_IMAGES = os.path.join(path, 'images/val')\n",
    "VAL_MASKS = os.path.join(path, 'labels/val')\n",
    "\n",
    "def check_path(path_name: str):\n",
    "    if not os.path.exists(path_name):\n",
    "        raise Exception(f\"Пути {path_name} не существует\")\n",
    "\n",
    "check_path(TRAIN_IMAGES)\n",
    "check_path(TRAIN_MASKS)\n",
    "check_path(VAL_IMAGES)\n",
    "check_path(VAL_MASKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20287b57",
   "metadata": {},
   "source": [
    "### Размеры выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a0bf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер тренировочной выборки: 7000\n",
      "Размер валидационной выборки: 1000\n"
     ]
    }
   ],
   "source": [
    "TRAIN_IMAGES_LIST = os.listdir(TRAIN_IMAGES)\n",
    "TRAIN_MASKS_LIST = os.listdir(TRAIN_MASKS)\n",
    "\n",
    "VAL_IMAGES_LIST = os.listdir(VAL_IMAGES)\n",
    "VAL_MASKS_LIST = os.listdir(VAL_MASKS)\n",
    "\n",
    "assert len(TRAIN_IMAGES_LIST) == len(TRAIN_MASKS_LIST)\n",
    "assert len(VAL_IMAGES_LIST) == len(VAL_MASKS_LIST)\n",
    "\n",
    "print(\"Размер тренировочной выборки:\", len(TRAIN_IMAGES_LIST))\n",
    "print(\"Размер валидационной выборки:\", len(VAL_IMAGES_LIST))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c97cdbe",
   "metadata": {},
   "source": [
    "Создаём класс для работы с датасетом. Хранение, трансформация и получение файлов из датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4904e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BDD100KDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, mask_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.images = sorted(os.listdir(image_dir))\n",
    "        self.masks = sorted(os.listdir(mask_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "        \n",
    "        mask = np.array(mask)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb804d",
   "metadata": {},
   "source": [
    "Функция для отображения примеров изображения и маски из датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935e701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(dataset: BDD100KDataset, cnt_images: int = 5):\n",
    "    plt.figure(figsize=(14, 5 * cnt_images))\n",
    "\n",
    "    for idx in range(cnt_images):\n",
    "        image, mask = dataset[idx]\n",
    "\n",
    "        plt.subplot(cnt_images, 2, idx * 2 + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f'Изображение {idx}')\n",
    "\n",
    "        plt.subplot(cnt_images, 2, idx * 2 + 2)\n",
    "        plt.imshow(np.array(mask).squeeze())\n",
    "        plt.title(f'Маска {idx}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd686f7c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e48c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_dataset = BDD100KDataset(TRAIN_IMAGES, TRAIN_MASKS, transform=None, mask_transform=None)\n",
    "show_images(preview_dataset, cnt_images=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db39208",
   "metadata": {},
   "source": [
    "Аугментация данных. Производим детерменированные изменения, поэтому синхронность необязательна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256), interpolation=Image.NEAREST)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b42cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = BDD100KDataset(TRAIN_IMAGES, TRAIN_MASKS, transform=image_transform, mask_transform=mask_transform)\n",
    "val_dataset = BDD100KDataset(VAL_IMAGES, VAL_MASKS, transform=image_transform, mask_transform=mask_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd79d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define number of classes (BDD100K has 19 classes for semantic segmentation)\n",
    "NUM_CLASSES = 19\n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    iou_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in data_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            masks = masks.cpu().numpy()\n",
    "\n",
    "            for pred, mask in zip(preds, masks):\n",
    "                pred_flat = pred.flatten()\n",
    "                mask_flat = mask.flatten()\n",
    "                iou = jaccard_score(mask_flat, pred_flat, average='macro', labels=range(NUM_CLASSES), zero_division=0)\n",
    "                precision = precision_score(mask_flat, pred_flat, average='macro', labels=range(NUM_CLASSES), zero_division=0)\n",
    "                recall = recall_score(mask_flat, pred_flat, average='macro', labels=range(NUM_CLASSES), zero_division=0)\n",
    "                f1 = f1_score(mask_flat, pred_flat, average='macro', labels=range(NUM_CLASSES), zero_division=0)\n",
    "                iou_scores.append(iou)\n",
    "                precision_scores.append(precision)\n",
    "                recall_scores.append(recall)\n",
    "                f1_scores.append(f1)\n",
    "\n",
    "    return {\n",
    "        'mIoU': np.mean(iou_scores),\n",
    "        'Precision': np.mean(precision_scores),\n",
    "        'Recall': np.mean(recall_scores),\n",
    "        'F1': np.mean(f1_scores)\n",
    "    }\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    best_miou = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, masks in train_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        metrics = evaluate_model(model, val_loader, device)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "              f\"mIoU: {metrics['mIoU']:.4f}, Precision: {metrics['Precision']:.4f}, \"\n",
    "              f\"Recall: {metrics['Recall']:.4f}, F1: {metrics['F1']:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if metrics['mIoU'] > best_miou:\n",
    "            best_miou = metrics['mIoU']\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 2. Create and Evaluate Baseline\n",
    "# Define baseline model (U-Net with ResNet34 backbone)\n",
    "baseline_model = smp.Unet(\n",
    "    encoder_name='resnet34',\n",
    "    encoder_weights='imagenet',\n",
    "    in_channels=3,\n",
    "    classes=NUM_CLASSES\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(baseline_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train baseline\n",
    "print(\"Training Baseline U-Net...\")\n",
    "train_model(baseline_model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=device)\n",
    "\n",
    "# Evaluate baseline\n",
    "baseline_metrics = evaluate_model(baseline_model, val_loader, device)\n",
    "print(\"Baseline Metrics:\", baseline_metrics)\n",
    "\n",
    "# 3. Improve Baseline\n",
    "# Define improved transforms with augmentations\n",
    "improved_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Reload dataset with augmentations\n",
    "train_dataset_improved = BDD100KDataset(TRAIN_IMAGES, TRAIN_MASKS, transform=improved_transform, mask_transform=mask_transform)\n",
    "train_loader_improved = DataLoader(train_dataset_improved, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "# Define improved model (DeepLabV3+ with ResNet50 backbone)\n",
    "improved_model = smp.DeepLabV3Plus(\n",
    "    encoder_name='resnet50',\n",
    "    encoder_weights='imagenet',\n",
    "    in_channels=3,\n",
    "    classes=NUM_CLASSES\n",
    ").to(device)\n",
    "\n",
    "optimizer_improved = torch.optim.Adam(improved_model.parameters(), lr=0.0005)\n",
    "\n",
    "# Train improved model\n",
    "print(\"Training Improved DeepLabV3+...\")\n",
    "train_model(improved_model, train_loader_improved, val_loader, criterion, optimizer_improved, num_epochs=10, device=device)\n",
    "\n",
    "# Evaluate improved model\n",
    "improved_metrics = evaluate_model(improved_model, val_loader, device)\n",
    "print(\"Improved Metrics:\", improved_metrics)\n",
    "\n",
    "# Compare results\n",
    "print(\"Baseline vs Improved:\")\n",
    "print(f\"Baseline mIoU: {baseline_metrics['mIoU']:.4f}, Improved mIoU: {improved_metrics['mIoU']:.4f}\")\n",
    "print(f\"Baseline F1: {baseline_metrics['F1']:.4f}, Improved F1: {improved_metrics['F1']:.4f}\")\n",
    "\n",
    "# 4. Implement Custom Model\n",
    "class CustomUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=NUM_CLASSES):\n",
    "        super(CustomUNet, self).__init__()\n",
    "        def conv_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.encoder1 = conv_block(in_channels, 64)\n",
    "        self.encoder2 = conv_block(64, 128)\n",
    "        self.encoder3 = conv_block(128, 256)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.bottleneck = conv_block(256, 512)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = conv_block(512, 256)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = conv_block(256, 128)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = conv_block(128, 64)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool(enc1))\n",
    "        enc3 = self.encoder3(self.pool(enc2))\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool(enc3))\n",
    "        \n",
    "        # Decoder\n",
    "        dec3 = self.upconv3(bottleneck)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        \n",
    "        return self.final_conv(dec1)\n",
    "\n",
    "# Train custom model\n",
    "custom_model = CustomUNet(in_channels=3, out_channels=NUM_CLASSES).to(device)\n",
    "optimizer_custom = torch.optim.Adam(custom_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training Custom U-Net...\")\n",
    "train_model(custom_model, train_loader, val_loader, criterion, optimizer_custom, num_epochs=10, device=device)\n",
    "\n",
    "# Evaluate custom model\n",
    "custom_metrics = evaluate_model(custom_model, val_loader, device)\n",
    "print(\"Custom Model Metrics:\", custom_metrics)\n",
    "\n",
    "# Compare with baseline\n",
    "print(\"Baseline vs Custom:\")\n",
    "print(f\"Baseline mIoU: {baseline_metrics['mIoU']:.4f}, Custom mIoU: {custom_metrics['mIoU']:.4f}\")\n",
    "print(f\"Baseline F1: {baseline_metrics['F1']:.4f}, Custom F1: {custom_metrics['F1']:.4f}\")\n",
    "\n",
    "# Apply improved techniques to custom model\n",
    "custom_model_improved = CustomUNet(in_channels=3, out_channels=NUM_CLASSES).to(device)\n",
    "optimizer_custom_improved = torch.optim.Adam(custom_model_improved.parameters(), lr=0.0005)\n",
    "\n",
    "print(\"Training Improved Custom U-Net...\")\n",
    "train_model(custom_model_improved, train_loader_improved, val_loader, criterion, optimizer_custom_improved, num_epochs=10, device=device)\n",
    "\n",
    "# Evaluate improved custom model\n",
    "custom_improved_metrics = evaluate_model(custom_model_improved, val_loader, device)\n",
    "print(\"Improved Custom Model Metrics:\", custom_improved_metrics)\n",
    "\n",
    "# Compare with improved baseline\n",
    "print(\"Improved Baseline vs Improved Custom:\")\n",
    "print(f\"Improved Baseline mIoU: {improved_metrics['mIoU']:.4f}, Improved Custom mIoU: {custom_improved_metrics['mIoU']:.4f}\")\n",
    "print(f\"Improved Baseline F1: {improved_metrics['F1']:.4f}, Improved Custom F1: {custom_improved_metrics['F1']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
